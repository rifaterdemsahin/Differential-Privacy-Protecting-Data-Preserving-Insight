{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîí Differential Privacy: Privacy-Preserving Machine Learning\n",
        "\n",
        "**Core Concept**: Differential privacy provides mathematical guarantees that machine learning models trained on sensitive data don't leak information about individual training examples.\n",
        "\n",
        "## üéØ Privacy Through Noise\n",
        "1.  **The Problem**: ML models can memorize training data (privacy leak)\n",
        "2.  **The Solution**: Add calibrated noise during training\n",
        "3.  **The Guarantee**: If algorithm runs on two datasets differing by one record, outputs look nearly identical\n",
        "4.  **The Result**: Attacker can't determine if individual's data was in training set\n",
        "\n",
        "## üìä The Privacy-Utility Tradeoff\n",
        "-   **More privacy (lower Œµ)**: More noise ‚Üí Lower model accuracy\n",
        "-   **Less privacy (higher Œµ)**: Less noise ‚Üí Higher model accuracy\n",
        "-   **Optimal range**: Œµ between 0.5 and 10.0 depending on sensitivity\n",
        "\n",
        "## üåç Real-World Examples\n",
        "-   **Apple**: Emoji usage, Safari browsing, Health app data\n",
        "-   **US Census Bureau**: Population statistics with formal privacy guarantees\n",
        "-   **Google**: Federated learning with differential privacy\n",
        "-   **Healthcare**: Medical research without exposing patient data\n",
        "\n",
        "This notebook demonstrates both the **Laplace Mechanism** (for private statistics) and **DP-SGD** (for private model training)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Step 1: Setup & Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd\n",
        "\n",
        "# Set random seeds\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Load MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize and reshape\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "X_train = X_train.reshape(-1, 28, 28, 1)\n",
        "X_test = X_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_train_cat = keras.utils.to_categorical(y_train, 10)\n",
        "y_test_cat = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "print(\"\\n‚úÖ Data loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¢ Step 2: The Laplace Mechanism - Private Statistics\n",
        "\n",
        "Before training models, let's understand differential privacy through a simple example: computing the average age in a dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def laplace_mechanism(true_value, sensitivity, epsilon):\n",
        "    \"\"\"\n",
        "    Add Laplace noise to achieve differential privacy.\n",
        "    \n",
        "    Args:\n",
        "        true_value: The actual computed value\n",
        "        sensitivity: Maximum change in value from adding/removing one record\n",
        "        epsilon: Privacy budget (lower = more privacy)\n",
        "    \n",
        "    Returns:\n",
        "        Noisy value with Œµ-DP guarantee\n",
        "    \"\"\"\n",
        "    scale = sensitivity / epsilon\n",
        "    noise = np.random.laplace(loc=0, scale=scale)\n",
        "    return true_value + noise\n",
        "\n",
        "# Example: Private average age\n",
        "# Simulate age data\n",
        "ages = np.random.randint(18, 80, size=1000)\n",
        "true_avg_age = np.mean(ages)\n",
        "\n",
        "# Sensitivity: (max_age - min_age) / n = (80 - 18) / 1000 = 0.062\n",
        "sensitivity = (80 - 18) / len(ages)\n",
        "\n",
        "# Test different epsilon values\n",
        "epsilons = [0.1, 0.5, 1.0, 5.0, 10.0]\n",
        "\n",
        "print(f\"True average age: {true_avg_age:.2f} years\\n\")\n",
        "print(\"Privacy Budget (Œµ) | Private Average | Error | Privacy Level\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for eps in epsilons:\n",
        "    private_avg = laplace_mechanism(true_avg_age, sensitivity, eps)\n",
        "    error = abs(private_avg - true_avg_age)\n",
        "    \n",
        "    if eps < 1.0:\n",
        "        privacy = \"Very Strong\"\n",
        "    elif eps < 5.0:\n",
        "        privacy = \"Strong\"\n",
        "    else:\n",
        "        privacy = \"Moderate\"\n",
        "    \n",
        "    print(f\"Œµ = {eps:5.1f}          | {private_avg:14.2f} | {error:5.2f} | {privacy}\")\n",
        "\n",
        "print(\"\\nüí° Notice: Lower epsilon (stronger privacy) ‚Üí Larger error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 3: Visualize Laplace Noise Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate many private averages to see noise distribution\n",
        "num_samples = 1000\n",
        "epsilon_test = 1.0\n",
        "\n",
        "private_averages = [laplace_mechanism(true_avg_age, sensitivity, epsilon_test) \n",
        "                   for _ in range(num_samples)]\n",
        "\n",
        "# Plot distribution\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(private_averages, bins=50, alpha=0.7, edgecolor='black')\n",
        "plt.axvline(true_avg_age, color='red', linestyle='--', linewidth=2, label='True Value')\n",
        "plt.xlabel('Private Average Age', fontsize=11)\n",
        "plt.ylabel('Frequency', fontsize=11)\n",
        "plt.title(f'Distribution of Private Averages (Œµ={epsilon_test})', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Compare different epsilons\n",
        "plt.subplot(1, 2, 2)\n",
        "for eps in [0.5, 1.0, 5.0]:\n",
        "    samples = [laplace_mechanism(true_avg_age, sensitivity, eps) for _ in range(1000)]\n",
        "    plt.hist(samples, bins=50, alpha=0.5, label=f'Œµ={eps}')\n",
        "plt.axvline(true_avg_age, color='red', linestyle='--', linewidth=2, label='True Value')\n",
        "plt.xlabel('Private Average Age', fontsize=11)\n",
        "plt.ylabel('Frequency', fontsize=11)\n",
        "plt.title('Noise Distribution Across Different Œµ Values', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Visualization shows:\")\n",
        "print(\"   - True value is hidden in a 'cloud' of noise\")\n",
        "print(\"   - Smaller epsilon ‚Üí Wider noise distribution ‚Üí Stronger privacy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è Step 4: Train Baseline Model (No Privacy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    \"\"\"Create a simple CNN for MNIST.\"\"\"\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create and train baseline model\n",
        "print(\"Training baseline model (no privacy)...\\n\")\n",
        "\n",
        "baseline_model = create_model()\n",
        "baseline_model.compile(optimizer='adam', \n",
        "                      loss='categorical_crossentropy', \n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "history_baseline = baseline_model.fit(\n",
        "    X_train, y_train_cat,\n",
        "    epochs=3,\n",
        "    batch_size=256,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "test_loss, baseline_acc = baseline_model.evaluate(X_test, y_test_cat, verbose=0)\n",
        "print(f\"\\n‚úÖ Baseline Model Accuracy: {baseline_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîê Step 5: Implement DP-SGD Components\n",
        "\n",
        "DP-SGD has three key steps:\n",
        "1.  Compute per-sample gradients (not averaged)\n",
        "2.  Clip each gradient to bounded L2 norm\n",
        "3.  Add calibrated noise to the average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clip_gradients(gradients, l2_norm_clip):\n",
        "    \"\"\"\n",
        "    Clip gradients to maximum L2 norm.\n",
        "    \n",
        "    Args:\n",
        "        gradients: List of gradient tensors\n",
        "        l2_norm_clip: Maximum allowed L2 norm\n",
        "    \n",
        "    Returns:\n",
        "        Clipped gradients\n",
        "    \"\"\"\n",
        "    clipped = []\n",
        "    for grad in gradients:\n",
        "        if grad is not None:\n",
        "            grad_norm = tf.norm(grad)\n",
        "            # Clip factor: min(1.0, clip_norm / actual_norm)\n",
        "            clip_factor = tf.minimum(1.0, l2_norm_clip / (grad_norm + 1e-10))\n",
        "            clipped.append(grad * clip_factor)\n",
        "        else:\n",
        "            clipped.append(None)\n",
        "    return clipped\n",
        "\n",
        "def add_noise_to_gradients(gradients, noise_multiplier, l2_norm_clip):\n",
        "    \"\"\"\n",
        "    Add Gaussian noise to gradients for differential privacy.\n",
        "    \n",
        "    Args:\n",
        "        gradients: List of gradient tensors\n",
        "        noise_multiplier: Scale of noise relative to clipping norm\n",
        "        l2_norm_clip: Gradient clipping norm\n",
        "    \n",
        "    Returns:\n",
        "        Noisy gradients\n",
        "    \"\"\"\n",
        "    noise_stddev = noise_multiplier * l2_norm_clip\n",
        "    \n",
        "    noisy = []\n",
        "    for grad in gradients:\n",
        "        if grad is not None:\n",
        "            noise = tf.random.normal(tf.shape(grad), mean=0.0, stddev=noise_stddev)\n",
        "            noisy.append(grad + noise)\n",
        "        else:\n",
        "            noisy.append(None)\n",
        "    return noisy\n",
        "\n",
        "print(\"‚úÖ DP-SGD helper functions defined:\")\n",
        "print(\"   - clip_gradients(): Bounds gradient norms\")\n",
        "print(\"   - add_noise_to_gradients(): Adds calibrated Gaussian noise\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ°Ô∏è Step 6: Train Model with DP-SGD\n",
        "\n",
        "Now we train a model with differential privacy guarantees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_dp_model(X_train, y_train, epochs=3, batch_size=256, \n",
        "                   l2_norm_clip=1.0, noise_multiplier=1.1):\n",
        "    \"\"\"\n",
        "    Train a model with DP-SGD.\n",
        "    \n",
        "    Args:\n",
        "        X_train, y_train: Training data\n",
        "        epochs: Number of training epochs\n",
        "        batch_size: Batch size\n",
        "        l2_norm_clip: Gradient clipping norm\n",
        "        noise_multiplier: Noise scale parameter\n",
        "    \n",
        "    Returns:\n",
        "        Trained model\n",
        "    \"\"\"\n",
        "    model = create_model()\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "    loss_fn = keras.losses.CategoricalCrossentropy()\n",
        "    \n",
        "    # Prepare dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "    dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "    \n",
        "    print(f\"Training with DP-SGD:\")\n",
        "    print(f\"  L2 norm clip: {l2_norm_clip}\")\n",
        "    print(f\"  Noise multiplier: {noise_multiplier}\")\n",
        "    print(f\"  Batch size: {batch_size}\\n\")\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        epoch_loss = []\n",
        "        epoch_acc = []\n",
        "        \n",
        "        for step, (batch_x, batch_y) in enumerate(dataset):\n",
        "            with tf.GradientTape() as tape:\n",
        "                predictions = model(batch_x, training=True)\n",
        "                loss = loss_fn(batch_y, predictions)\n",
        "            \n",
        "            # Compute gradients\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            \n",
        "            # DP-SGD modifications\n",
        "            # 1. Clip gradients\n",
        "            clipped_gradients = clip_gradients(gradients, l2_norm_clip)\n",
        "            \n",
        "            # 2. Add noise\n",
        "            noisy_gradients = add_noise_to_gradients(clipped_gradients, \n",
        "                                                    noise_multiplier, \n",
        "                                                    l2_norm_clip)\n",
        "            \n",
        "            # 3. Apply gradients\n",
        "            optimizer.apply_gradients(zip(noisy_gradients, model.trainable_variables))\n",
        "            \n",
        "            # Track metrics\n",
        "            epoch_loss.append(loss.numpy())\n",
        "            acc = tf.reduce_mean(tf.cast(\n",
        "                tf.equal(tf.argmax(predictions, axis=1), tf.argmax(batch_y, axis=1)),\n",
        "                tf.float32\n",
        "            ))\n",
        "            epoch_acc.append(acc.numpy())\n",
        "            \n",
        "            if step % 50 == 0:\n",
        "                print(f\"  Step {step}: Loss={loss.numpy():.4f}, Acc={acc.numpy():.4f}\")\n",
        "        \n",
        "        print(f\"  Epoch {epoch + 1} - Avg Loss: {np.mean(epoch_loss):.4f}, Avg Acc: {np.mean(epoch_acc):.4f}\\n\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Train DP model with moderate privacy (epsilon ~ 1.0)\n",
        "dp_model = train_dp_model(X_train, y_train_cat, \n",
        "                         epochs=3, \n",
        "                         batch_size=256, \n",
        "                         l2_norm_clip=1.0, \n",
        "                         noise_multiplier=1.1)\n",
        "\n",
        "# Evaluate\n",
        "dp_loss, dp_acc = dp_model.evaluate(X_test, y_test_cat, verbose=0)\n",
        "print(f\"\\n‚úÖ DP Model Accuracy: {dp_acc*100:.2f}%\")\n",
        "print(f\"Baseline Accuracy: {baseline_acc*100:.2f}%\")\n",
        "print(f\"Accuracy Gap: {(baseline_acc - dp_acc)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìê Step 7: Privacy Budget Calculation (Simplified)\n",
        "\n",
        "Calculate the privacy budget (epsilon) spent during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_epsilon_simple(steps, noise_multiplier, batch_size, dataset_size, delta=1e-5):\n",
        "    \"\"\"\n",
        "    Simplified epsilon calculation using strong composition.\n",
        "    \n",
        "    Note: Real implementations should use RDP accountant for accuracy.\n",
        "    This is an approximation for demonstration.\n",
        "    \"\"\"\n",
        "    # Sampling probability\n",
        "    q = batch_size / dataset_size\n",
        "    \n",
        "    # Per-step epsilon (simplified)\n",
        "    # Real formula involves complex privacy accounting\n",
        "    epsilon_per_step = q / noise_multiplier\n",
        "    \n",
        "    # Total epsilon with advanced composition (approximate)\n",
        "    epsilon_total = epsilon_per_step * np.sqrt(2 * steps * np.log(1 / delta))\n",
        "    \n",
        "    return epsilon_total\n",
        "\n",
        "# Calculate privacy budget for our DP model\n",
        "num_epochs = 3\n",
        "batch_size = 256\n",
        "dataset_size = len(X_train)\n",
        "steps_per_epoch = dataset_size // batch_size\n",
        "total_steps = num_epochs * steps_per_epoch\n",
        "noise_multiplier = 1.1\n",
        "delta = 1e-5\n",
        "\n",
        "epsilon = calculate_epsilon_simple(total_steps, noise_multiplier, \n",
        "                                  batch_size, dataset_size, delta)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PRIVACY BUDGET ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total training steps:     {total_steps}\")\n",
        "print(f\"Noise multiplier:         {noise_multiplier}\")\n",
        "print(f\"Gradient clipping norm:   1.0\")\n",
        "print(f\"Delta (Œ¥):                {delta}\")\n",
        "print(f\"\\nPrivacy Budget (Œµ):       {epsilon:.2f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if epsilon < 1.0:\n",
        "    privacy_level = \"Very Strong Privacy\"\n",
        "elif epsilon < 5.0:\n",
        "    privacy_level = \"Strong Privacy\"\n",
        "elif epsilon < 10.0:\n",
        "    privacy_level = \"Moderate Privacy\"\n",
        "else:\n",
        "    privacy_level = \"Weak Privacy\"\n",
        "\n",
        "print(f\"\\nPrivacy Level: {privacy_level}\")\n",
        "print(f\"\\nüí° Interpretation: With Œµ={epsilon:.2f}, the model satisfies\")\n",
        "print(f\"   ({epsilon:.2f}, {delta})-differential privacy.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¨ Step 8: Privacy-Utility Tradeoff Analysis\n",
        "\n",
        "Train models with different epsilon values to see the tradeoff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Exploring privacy-utility tradeoff...\\n\")\n",
        "\n",
        "# Test different noise multipliers (inverse relationship with epsilon)\n",
        "noise_multipliers = [0.5, 0.8, 1.1, 1.5, 2.0]\n",
        "results = []\n",
        "\n",
        "for nm in noise_multipliers:\n",
        "    print(f\"Training with noise multiplier = {nm}...\")\n",
        "    \n",
        "    # Train model\n",
        "    model_temp = train_dp_model(X_train[:30000], y_train_cat[:30000],  # Use subset for speed\n",
        "                               epochs=2, \n",
        "                               batch_size=256, \n",
        "                               l2_norm_clip=1.0, \n",
        "                               noise_multiplier=nm)\n",
        "    \n",
        "    # Evaluate\n",
        "    _, acc = model_temp.evaluate(X_test, y_test_cat, verbose=0)\n",
        "    \n",
        "    # Calculate epsilon\n",
        "    steps = 2 * (30000 // 256)\n",
        "    eps = calculate_epsilon_simple(steps, nm, 256, 30000, delta=1e-5)\n",
        "    \n",
        "    results.append({\n",
        "        'Noise Multiplier': nm,\n",
        "        'Epsilon': eps,\n",
        "        'Accuracy': acc * 100\n",
        "    })\n",
        "    \n",
        "    print(f\"  Epsilon: {eps:.2f}, Accuracy: {acc*100:.2f}%\\n\")\n",
        "\n",
        "# Create results table\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PRIVACY-UTILITY TRADEOFF\")\n",
        "print(\"=\"*60)\n",
        "print(df_results.to_string(index=False))\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 9: Visualize Privacy-Utility Tradeoff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot tradeoff curve\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "epsilons = df_results['Epsilon'].values\n",
        "accuracies = df_results['Accuracy'].values\n",
        "\n",
        "# Add baseline (no privacy)\n",
        "epsilons_plot = np.append(epsilons, [100])  # Represent infinity as 100 for plotting\n",
        "accuracies_plot = np.append(accuracies, [baseline_acc * 100])\n",
        "\n",
        "ax.plot(epsilons_plot[:-1], accuracies_plot[:-1], 'o-', linewidth=2, \n",
        "        markersize=8, label='DP Models', color='blue')\n",
        "ax.scatter([epsilons_plot[-1]], [accuracies_plot[-1]], \n",
        "          s=100, color='red', marker='*', label='No Privacy (Baseline)', zorder=5)\n",
        "\n",
        "ax.set_xlabel('Privacy Budget (Œµ)', fontsize=12)\n",
        "ax.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "ax.set_title('Privacy-Utility Tradeoff Curve', fontsize=14)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend(fontsize=11)\n",
        "\n",
        "# Annotate privacy zones\n",
        "ax.axvspan(0, 1, alpha=0.1, color='green', label='Strong Privacy')\n",
        "ax.axvspan(1, 5, alpha=0.1, color='yellow')\n",
        "ax.axvspan(5, 10, alpha=0.1, color='orange')\n",
        "\n",
        "# Add text annotations\n",
        "ax.text(0.5, accuracies_plot.min() + 1, 'Strong\\nPrivacy', \n",
        "       ha='center', fontsize=9, color='darkgreen')\n",
        "ax.text(3, accuracies_plot.min() + 1, 'Moderate\\nPrivacy', \n",
        "       ha='center', fontsize=9, color='darkorange')\n",
        "ax.text(7.5, accuracies_plot.min() + 1, 'Weak\\nPrivacy', \n",
        "       ha='center', fontsize=9, color='darkred')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Key Insight:\")\n",
        "print(\"   Lower epsilon (stronger privacy) ‚Üí Lower accuracy\")\n",
        "print(\"   The optimal epsilon depends on your privacy requirements!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üïµÔ∏è Step 10: Membership Inference Attack Test\n",
        "\n",
        "Test if an attacker can determine if a specific example was in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def membership_inference_attack(model, train_data, train_labels, test_data, test_labels):\n",
        "    \"\"\"\n",
        "    Simple membership inference attack based on prediction confidence.\n",
        "    \n",
        "    Returns:\n",
        "        Attack accuracy (50% = random guessing = perfect privacy)\n",
        "    \"\"\"\n",
        "    # Get prediction confidences\n",
        "    train_preds = model.predict(train_data, verbose=0)\n",
        "    test_preds = model.predict(test_data, verbose=0)\n",
        "    \n",
        "    # Extract confidence for correct class\n",
        "    train_confidences = [train_preds[i, train_labels[i]] for i in range(len(train_labels))]\n",
        "    test_confidences = [test_preds[i, test_labels[i]] for i in range(len(test_labels))]\n",
        "    \n",
        "    # Threshold-based attack: high confidence ‚Üí likely in training set\n",
        "    threshold = np.median(train_confidences + test_confidences)\n",
        "    \n",
        "    # Classify based on threshold\n",
        "    train_predictions = np.array(train_confidences) > threshold  # Should be True\n",
        "    test_predictions = np.array(test_confidences) > threshold    # Should be False\n",
        "    \n",
        "    # Calculate attack accuracy\n",
        "    train_correct = np.mean(train_predictions)\n",
        "    test_correct = np.mean(~test_predictions)\n",
        "    attack_accuracy = (train_correct + test_correct) / 2\n",
        "    \n",
        "    return attack_accuracy * 100\n",
        "\n",
        "# Test on baseline model (no privacy)\n",
        "print(\"Testing membership inference attack...\\n\")\n",
        "\n",
        "train_subset = X_train[:1000]\n",
        "train_labels_subset = y_train[:1000]\n",
        "test_subset = X_test[:1000]\n",
        "test_labels_subset = y_test[:1000]\n",
        "\n",
        "baseline_attack_acc = membership_inference_attack(\n",
        "    baseline_model, train_subset, train_labels_subset, \n",
        "    test_subset, test_labels_subset\n",
        ")\n",
        "\n",
        "dp_attack_acc = membership_inference_attack(\n",
        "    dp_model, train_subset, train_labels_subset, \n",
        "    test_subset, test_labels_subset\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MEMBERSHIP INFERENCE ATTACK RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Baseline Model (No Privacy):  {baseline_attack_acc:.1f}%\")\n",
        "print(f\"DP Model (Œµ‚âà{epsilon:.1f}):            {dp_attack_acc:.1f}%\")\n",
        "print(f\"Random Guessing (Perfect):    50.0%\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüí° Interpretation:\")\n",
        "print(\"   - High attack accuracy (>60%) = Privacy leak\")\n",
        "print(\"   - Near 50% = Strong privacy (attacker can't do better than random)\")\n",
        "\n",
        "if dp_attack_acc < baseline_attack_acc - 5:\n",
        "    print(\"\\n‚úÖ Differential privacy successfully reduced privacy leakage!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Consider increasing noise multiplier for stronger privacy.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Summary\n",
        "\n",
        "### What We Demonstrated:\n",
        "‚úÖ **Laplace Mechanism**: Adding calibrated noise to statistics for privacy  \n",
        "‚úÖ **DP-SGD**: Training neural networks with formal privacy guarantees  \n",
        "‚úÖ **Privacy Budget**: Epsilon quantifies privacy loss (lower = stronger privacy)  \n",
        "‚úÖ **Privacy-Utility Tradeoff**: More privacy ‚Üí More noise ‚Üí Lower accuracy  \n",
        "‚úÖ **Membership Inference**: DP models resist privacy attacks  \n",
        "\n",
        "### Key Concepts:\n",
        "\n",
        "#### Differential Privacy Definition\n",
        "An algorithm M satisfies **Œµ-differential privacy** if for any two datasets D1, D2 differing by one record:\n",
        "```\n",
        "Pr[M(D1) = o] ‚â§ e^Œµ √ó Pr[M(D2) = o]\n",
        "```\n",
        "**Meaning**: Outputs are nearly identical whether your data is included or not.\n",
        "\n",
        "#### DP-SGD Algorithm\n",
        "```python\n",
        "# Standard SGD\n",
        "gradients = compute_gradients(batch)\n",
        "apply_gradients(gradients)\n",
        "\n",
        "# DP-SGD\n",
        "per_sample_grads = compute_per_sample_gradients(batch)\n",
        "clipped_grads = clip_gradients(per_sample_grads, norm=1.0)\n",
        "noisy_grads = add_gaussian_noise(clipped_grads, scale=noise_multiplier)\n",
        "apply_gradients(noisy_grads)\n",
        "```\n",
        "\n",
        "### Privacy Parameters:\n",
        "\n",
        "#### Epsilon (Œµ) - Privacy Budget\n",
        "-   **Œµ < 1.0**: Very strong privacy, significant accuracy loss\n",
        "-   **Œµ ‚âà 1.0**: Strong privacy, moderate accuracy loss (~5-10%)\n",
        "-   **Œµ = 5.0**: Moderate privacy, small accuracy loss (~2-5%)\n",
        "-   **Œµ > 10.0**: Weak privacy, minimal accuracy loss\n",
        "\n",
        "#### Delta (Œ¥) - Failure Probability\n",
        "-   Typical value: **1/n¬≤** where n is dataset size\n",
        "-   For 10,000 samples: Œ¥ = 1e-5\n",
        "-   Privacy guarantee holds \"except with probability Œ¥\"\n",
        "\n",
        "#### Noise Multiplier\n",
        "-   Controls amount of noise added to gradients\n",
        "-   Higher value ‚Üí Lower epsilon ‚Üí Stronger privacy\n",
        "-   Typical range: 0.5 to 2.0\n",
        "\n",
        "#### Gradient Clipping Norm\n",
        "-   Bounds influence of any single training example\n",
        "-   Typical value: 0.5 to 1.5\n",
        "-   Too low: slow learning; too high: weak privacy\n",
        "\n",
        "### Real-World Applications:\n",
        "\n",
        "#### Apple's Differential Privacy\n",
        "-   Emoji usage patterns\n",
        "-   Safari browsing data\n",
        "-   Health app statistics\n",
        "-   **Method**: Local differential privacy (noise added on-device)\n",
        "\n",
        "#### US Census Bureau\n",
        "-   2020 Census data release\n",
        "-   Population counts by demographics\n",
        "-   **Goal**: Protect individual responses while releasing aggregate statistics\n",
        "\n",
        "#### Healthcare Research\n",
        "-   Clinical trial results\n",
        "-   Epidemic modeling\n",
        "-   Medical image analysis\n",
        "-   **Requirement**: HIPAA compliance + research utility\n",
        "\n",
        "### The Fundamental Tradeoff:\n",
        "```\n",
        "Privacy ‚Üî Utility\n",
        "\n",
        "Stronger Privacy (lower Œµ):       Weaker Privacy (higher Œµ):\n",
        "  + Better protection                + Higher accuracy\n",
        "  + Attack resistance                + Faster training\n",
        "  - Lower accuracy                   - Privacy leakage risk\n",
        "  - More noise                       - Less noise\n",
        "```\n",
        "\n",
        "### When to Use Differential Privacy:\n",
        "‚úÖ Training on sensitive personal data (health, finance, personal info)  \n",
        "‚úÖ Regulatory requirements (GDPR, HIPAA, CCPA)  \n",
        "‚úÖ Public data releases (census, research datasets)  \n",
        "‚úÖ Federated learning scenarios  \n",
        "‚úÖ When formal privacy guarantees are required  \n",
        "\n",
        "### Best Practices:\n",
        "1.  **Set privacy budget (epsilon) before training** based on sensitivity\n",
        "2.  **Use established libraries** (TensorFlow Privacy, Opacus) for correct implementation\n",
        "3.  **Monitor epsilon during training** and stop when budget is exhausted\n",
        "4.  **Test with membership inference** to verify privacy guarantees\n",
        "5.  **Balance privacy and utility** based on application requirements\n",
        "6.  **Increase dataset size** when possible (larger n ‚Üí better tradeoff)\n",
        "7.  **Don't reuse privacy budget** across multiple experiments\n",
        "\n",
        "**Differential privacy is the gold standard for privacy-preserving machine learning with mathematical guarantees.**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
